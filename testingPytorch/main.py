import torch
import torch.nn as nn
import torch.optim as optim
import os
from transformers import AutoTokenizer

# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
text = [(
    "–ú–∞–º–æ—á–∫–∞ —Ç—ã –º–æ—è —Ö–æ—Ä–æ—à–∞—è, —Ö–æ—Ä–æ—à–∞—è —Ç—ã –º–æ—è –º–∞–º–∞, –º–∞–º—É–ª—è —Ç—ã –º–æ—è —Ö–æ—Ä–æ—à–∞—è! "
    "–≠—Ö —Ç—ã, –º–∞—Ç—é—à–∫–∞ —Ç—ã –º–æ—è! –ú–∞–º–æ—á–∫–∞ —Ç—ã –º–æ—è! –ß—Ç–æ –∂ —Ç—ã –Ω–∞—Ç–≤–æ—Ä–∏–ª–∞-—Ç–æ?! "
    "–•–æ–ª–æ–¥–Ω–æ–π —Ö–æ—á–µ—à—å —Å—Ç–∞—Ç—å —á—Ç–æ –ª–∏?! –û—Å—Ç—ã–≤—à–∏–µ —â–∏ —Ö–æ—á–µ—à—å –º–Ω–µ –ø—Ä–µ–ø–æ–¥–∞—Ç—å —á—Ç–æ –ª–∏? "
    "–ú–∞–º! –¢—é—Ä—è, —Ç—ã –∫–∞—à–∞-–º–∞–ª–∞—à–∞ –≤ –ª–µ–ø–µ—Å—Ç–æ—á–∫–∏! –≠—Ö —Ç—ã! –ú–∞—Ç—å-—Ç–æ –º–æ—è! "
    "–õ–µ—Å–æ–º –ø–æ—à–ª–∏ –±—ã, –ø–æ–ª–µ–º –ø–æ—à–ª–∏ –±—ã! –°–µ–ª–∏ –±—ã! –°–ø–æ–∫–æ–π–Ω–æ, —Å–ø—Ä–æ—Å–∏–ª –±—ã, "
    "–ø–æ–∫–∞–∫–∞—Ç—å –º–æ–∂–Ω–æ, —Ç—ã –±—ã —Å–∫–∞–∑–∞–ª–∞ –±—ã: –∏–¥–∏ –∏ –ø–æ–∫–∞–∫–∞–π –ø–æ–¥ –∫—É—Å—Ç–∏–∫-—Ç–æ! "
    "–ò —è –± –ø–æ–∫–∞–∫–∞–ª –±—ã! –ü–æ—Å—Ä–∞–ª –±—ã —Ç–∞–º! –û–±–æ—Å—Ä–∞–ª –≤—Å—ë! "
    "–ì–æ–≤–Ω–æ –±—ã –≤—Å–µ –≤—ã—Ç–µ—Ä –ø–∞–ª—å—Ü–µ–º, –≤—ã—Ç–µ—Ä –±—ã! –ú–∞–º—É–ª–µ—á–∫–∞, –ø–æ—Ç–æ–º –±—ã –ª–∏—Å—Ç–æ—á–∫–æ–º –±—ã –≤—ã—Ç–µ—Ä! "
    "–Ø –±—ã –≤–µ—Å—å –ª–∏—Å—Ç–æ—á–∫–æ–º –±—ã–ª –±—ã —É–º—ã—Ç—ã–π, –±—ã–ª –±—ã!!!"
)]# —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –±—ã–ª –±–∞—Ç—á - 1
#forward pass
encoded = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
input_ids = encoded['input_ids']           # [batch, seq_len]
attention_mask = encoded['attention_mask'] # [batch, seq_len]

torch.save(input_ids, "input_ids.pt")
torch.save(attention_mask, "attention_mask.pt")


print("Input IDs:", input_ids)
print("Shape:", input_ids.shape)  # torch.Size([1, seq_len])

# 2. –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
vocab_size = tokenizer.vocab_size  # –ª—É—á—à–µ –±—Ä–∞—Ç—å –ø—Ä—è–º–æ –∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
embedding_dim = 768
embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)

# 3. –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞
embedded = embedding_layer(input_ids)  # batch, seq_len, embedding_dim —ç—Ç–æ —Å–∞–º —Ç–µ–Ω–∑–µ—Ä –µ—Å —á–µ –µ–ø—Ç–∞
print("Embedded shape:", embedded.shape)
torch.save(embedded, "tensor.pt")#—Å–ø–∞—Å–∏–±–æ —Ç–æ–º—É —á–µ–ª–æ–≤–µ–∫—É!

print(embedded[0,0:20])

#—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
src = embedded.transpose(0, 1) #—Ç–µ–ø–µ—Ä—å seq_len –∑–∞—Ç–µ–º batch –∑–∞—Ç–µ–º em bedding_dim - —ç—Ç–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ —Ç—Ä–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞

transformerLayer = nn.TransformerEncoderLayer(
    d_model=embedding_dim,
    nhead=12,#–∞—Ö—Ä–∏–Ω–µ—Ç—å –∫–∞–∫ –≤—Å–µ –ø—Ä–æ—Å—Ç–æ!!!! —ç—Ç–æ —Ç–∏–ø–æ —è —â–∞—Å 12 –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏–º–µ—é!!!
    dim_feedforward=2048, # –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN
    dropout=0.1, #Dropout - –æ—Ç–∫–ª—é—á–∞–µ—Ç 10% –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å–ª—É—á–∞–π–Ω–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    batch_first=False#–º–µ—Å—Ç–∞–º–∏ —É–∂–µ –∏–∑–º–µ–Ω–µ–Ω —Ç—É—Ç - src = embedded.transpose(0, 1)

)
transformer_encoderLayer = nn.TransformerEncoder(transformerLayer,num_layers=2)
paddingMask = (attention_mask == 0)

outputTransformer0 = transformer_encoderLayer(
    src,
    src_key_padding_mask = paddingMask

)
outputTransformer = outputTransformer0.transpose(0, 1)# [S, B, E]
print(outputTransformer.shape)

#–õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
vocab_size = tokenizer.vocab_size
embedding_dim = outputTransformer.shape[2]

output_layer = nn.Linear(embedding_dim,vocab_size)

logitOutputLayer = output_layer(outputTransformer) # [batch, seq_len, vocab_size]

# –î–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
#probs = torch.softmax(logitOutputLayer, dim=-1)# softmax –ø–æ —Å–ª–æ–≤–∞—Ä—é


#–ø—Ä–æ—Å—Ç–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω
def decode_tokens(tokens):
    text = ""
    for t in tokens:
        if t.startswith("##"):
            text += t[2:]  # —É–±–∏—Ä–∞–µ–º ##
        else:
            text += " " + t
    return text.strip()

#–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ(—Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —Ç–æ –ø–æ—á–µ–º—É –±—ã –∏ –Ω–µ—Ç ? –∫–æ–Ω–µ—á–Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–æ —Å—É–Ω—É —Å—é–¥–∞ )

class LearnedPositionalEncoding(nn.Module):
    def __init__(self, max_len: int, embedding_dim: int):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, embedding_dim)# –≤ —á–µ–º –∑–∞–¥—É–º–∫–∞ ? —Å–æ–∑–¥–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä,–¥–∞ ?
        #–ø–æ –ø–æ —Ñ–æ—Ä–º–µ - [max_len, embedding_dim] –ø—Ä—è–º –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ
        #nn.Embedding –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ
        #—Å—Ç—Ä–æ–∫–∏ —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã. –í–µ—Å pos_embedding.weight –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±—É–¥–µ—Ç
        #–∏–º–µ—Ç—å requires_grad=True, –∑–Ω–∞—á–∏—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ–π–¥—É—Ç –≤ —ç—Ç–∏ –≤–µ—Å–∞ –∏ –æ–Ω–∏
        #–±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º.
    def forward(self, x):
        # x: [batch, seq_len, embedding_dim] - –Ω–∞—à —Ç–µ–Ω–∑–æ—Ä
        seq_len = x.size(1)#–±–µ—Ä—ë–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ.
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]
        pos_embed = self.pos_embedding(positions)  # [1, seq_len, embedding_dim]
        return x + pos_embed # —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —Ñ–æ—Ä–º–µ [1, seq_len, embedding_dim]

#–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ(forward pass)

referense = tokenizer(
    ["–¢—ã –∂–∏–≤–µ—à—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å! –•–æ—á–µ—à—å, —è –Ω–∞ –æ–¥–Ω–æ–π –Ω–æ–≥–µ –ø–æ—Å—Ç–æ—é –∫–∞–∫ —Ü–∞–ø–ª—è, —Ö–æ—á–µ—à—å?"],
    padding='max_length', truncation=True, max_length=input_ids.shape[1], return_tensors='pt'
)
target_ids = referense['input_ids']

torch.save(target_ids, "inputReferense_ids.pt")



criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)


loss = criterion(
    logitOutputLayer.view(-1, vocab_size),
    target_ids.view(-1)
)
pos_encoding = LearnedPositionalEncoding(max_len=input_ids.shape[1], embedding_dim=embedding_dim)
optimizer = torch.optim.Adam(list(embedding_layer.parameters()) +
                             list(transformer_encoderLayer.parameters()) +
                             list(pos_encoding.parameters()) +
                             list(output_layer.parameters()), lr=1e-4)

checkpoint_path = "model_checkpoint.pt"

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    embedding_layer.load_state_dict(checkpoint['embedding_state'])
    pos_encoding.load_state_dict(checkpoint['pos_encoding_state'])  # üëà –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∂–µ
    transformer_encoderLayer.load_state_dict(checkpoint['transformer_state'])
    output_layer.load_state_dict(checkpoint['output_state'])
    optimizer.load_state_dict(checkpoint['optimizer_state'])
    start_epoch = checkpoint['epoch'] + 1
    print(f" –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —ç–ø–æ—Ö–∏ {start_epoch}")
else:
    start_epoch = 0
    print(" –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")

epochNum = 10
for epoch in range(epochNum):
    optimizer.zero_grad()
    epochmy = start_epoch + epoch
    embedded = embedding_layer(input_ids)
    embedded = pos_encoding(embedded)
    src = embedded.transpose(0, 1)

    outputTransformer = transformer_encoderLayer(src, src_key_padding_mask=(attention_mask == 0))
    outputTransformer = outputTransformer.transpose(0, 1)  # –æ–±—Ä–∞—Ç–Ω–æ [batch, seq_len, embedding_dim]

    logits = output_layer(outputTransformer)
    loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))
    before = pos_encoding.pos_embedding.weight.clone()
    loss.backward()
    optimizer.step()  # –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
    after = pos_encoding.pos_embedding.weight
    print(f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ pos_encoding: {(after - before).abs().sum():.6f}")
    print("Loss:", loss.item())

    # –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è (–∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞, —á—Ç–æ–±—ã —Å–º–æ—Ç—Ä–µ—Ç—å –¥–∏–Ω–∞–º–∏–∫—É)
    with torch.no_grad():
        embedded = embedding_layer(input_ids)
        embedded = pos_encoding(embedded)
        src = embedded.transpose(0, 1)
        outputTransformer = transformer_encoderLayer(src, src_key_padding_mask=(attention_mask == 0))
        outputTransformer = outputTransformer.transpose(0, 1)
        logits = output_layer(outputTransformer)  # [batch, seq_len, vocab_size]

        # –ë–µ—Ä—ë–º —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è
        predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch, seq_len]

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
        predicted_text = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)
        print("Predicted text:", predicted_text[0])

        print("Loss before backward:", loss.item())

for name, param in pos_encoding.named_parameters():
    print(name, param.shape, param.requires_grad)

print(f"Epoch [{epochmy + 1}/{start_epoch + epochNum}] ‚Äî Loss: {loss.item():.6f}")
torch.save({
    'embedding_state': embedding_layer.state_dict(),
    'pos_encoding_state': pos_encoding.state_dict(),
    'transformer_state': transformer_encoderLayer.state_dict(),
    'output_state': output_layer.state_dict(),
    'optimizer_state': optimizer.state_dict(),
    'epoch': epochmy
}, "model_checkpoint.pt")
